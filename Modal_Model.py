# -*- coding: utf-8 -*-

""" Modal_Model.py
Uses .csv files generated from supporting programs and predicts modal numbers based
on input parameters of frequency,length, width, magnitude, and phase of the 
electric and magnetic fields of the rectangular waveguide
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mUPZ9vfMmeq6QkX7xMjTCVxgPU2Ss1Xo
Date: April 28 2022
Author: Kate Antonov

Sources:
********************************************************************************
NOTE: The use of this program is limited to NON-COMMERCIAL usage only.
If the program code (or a modified version) is used in a scientific work,
then reference should be made to the following:
[1] ECE 504 Machine Learning for Electromagnetics, Instructor: Dr. Ata Zadehgol, Spring 2022.
[2] Pozar, D. M.(2005) Microwave Engineering. John Wiley and Sons.
[3] Géron, A. (2020). Hands-on machine learning with scikit-learn, Keras, and tensorflow: Concepts, tools, and techniques to build Intelligent Systems. O'Reilly. 
[4] Avinash Navlani, datacamp. August 2nd, 2018. Accessed on March 2nd 2022.
[Online]. Available: https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn
[5] scikit-learn, "sklearn.model_selection.GridSearchCV", https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
[6]  Malay Agarwal, Real Python. Accessed on March 2nd 2022.
[Online]. Available: https://realpython.com/python-data-cleaning-numpy-pandas/
********************************************************************************
"""
import warnings

# Commented out IPython magic to ensure Python compatibility.
# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

# Common imports
import numpy as np
import os

# To plot pretty figures
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "Project_2"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

"""LOOKING AT DATA SET"""

from google.colab import files
uploaded = files.upload()

import io
import pandas as pd
#df2 = pd.read_csv(io.BytesIO(uploaded['Data_FINAL_1.csv']))#Gaussian
df2 = pd.read_csv(io.BytesIO(uploaded['Data_Generated_Ex_normal_mn.csv']))#Randomized
print(df2)
# Dataset is now stored in a Pandas Dataframe

df2.info()

df2.head()

df2.describe()

df2["x"].value_counts()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
df2.hist(bins=50, figsize=(20,15))
save_fig("attribute_histogram_plots")
plt.show()

"""CLEANING DATA SET

"""

from sklearn import preprocessing
import pandas as pd

scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
names = df2.columns
d = scaler.fit_transform(df2)
scaled_df2 = pd.DataFrame(d, columns=names)
scaled_df2.head()

scaled_df2.isnull().sum().sort_values(ascending=False)

scaled_df2.isnull().sum().sort_values(ascending=False)

#drop row
no_magn = scaled_df2[scaled_df2['magn'].isnull()].index
#scaled_df2_Drop = scaled_df2.drop(no_magn,inplace = True)
#~ + .isnull()
scaled_df2[~scaled_df2['magn'].isnull()]
#dropna()
scaled_df2.dropna(subset=['magn'],inplace = True)
print(scaled_df2)
#drop row
no_phase = scaled_df2[scaled_df2['phase'].isnull()].index
#scaled_df2_Drop = scaled_df2.drop(no_magn,inplace = True)
#~ + .isnull()
scaled_df2[~scaled_df2['phase'].isnull()]
#dropna()
scaled_df2.dropna(subset=['phase'],inplace = True)
print(scaled_df2)

from sklearn import preprocessing
magn = scaled_df2.magn
phase = scaled_df2.phase
mag_phase = np.array([magn,phase])
x = scaled_df2.x
y = scaled_df2.y
m = scaled_df2.m
n = scaled_df2.n
all = np.array([magn,phase,x,y])
output = np.array([m,n])
print(magn)
print(phase)
print(x)
print(y)
def prepare_inputs(X_train):
	oe = preprocessing.OrdinalEncoder()
	oe.fit(X_train)
	X_train_enc = oe.transform(X_train)
	#X_test_enc = oe.transform(X_test)
  
	return X_train_enc

pre_magn_train_enc = prepare_inputs(all)
magn_train_enc = pre_magn_train_enc.T
print(magn_train_enc)

def prepare_outputs(y_train):
	oe = preprocessing.OrdinalEncoder()
	oe.fit(y_train)
	y_train_enc = oe.transform(y_train)
	#X_test_enc = oe.transform(X_test)
  
	return y_train_enc
pre_output_train_enc = prepare_outputs(output)
output_train_enc = pre_output_train_enc.T
print(output_train_enc)

"""STARTING TO SPLIT INTO TRAINING AND TESTING"""

# to make this notebook's output identical at every run
np.random.seed(42)

#train_set, test_set = split_train_test(scaled_df2, 0.5)
from sklearn.model_selection import train_test_split

y_data = scaled_df2.m
MinMaxScaler = preprocessing.MinMaxScaler()
#X_data_minmax = MinMaxScaler.fit_transform(x_data)
data = magn_train_enc
#data = pd.DataFrame(magn_train_enc,columns=['Magn', 'Phase','x','y'])
#print(data)
#print(y_data)
X_train, X_test, y_train, y_test = train_test_split(magn_train_enc, output_train_enc, test_size=0.2,random_state =42)
print(X_train)
print(X_test)
print(y_train)
print(y_test)
print(len(X_train))
print(len(X_test))
print(len(y_train))
print(len(y_test))

corr_matrix = scaled_df2.corr()

corr_matrix["magn"].sort_values(ascending=False)

# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas
from pandas.plotting import scatter_matrix

#attributes = ["m","n","Magn", "Phase","x","y"]
attributes= ["m","n","magn", "phase"]
scatter_matrix(scaled_df2[attributes], figsize=(12, 8))
save_fig("scatter_matrix_plot_frequency")

from logging import log
scaled_df2.plot(kind="scatter", x="phase", y="m",
             alpha=0.1)
#plt.axis([0, 1, 0, 20e9])
#plt.yscale("log")
#plt.xscale("log")
save_fig("magnitude_vs_frequency_scatterplot")

"""KNN CLASSIFICATION"""

import pandas as pd

scaled_df2.head()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
#knn = KNeighborsClassifier(n_neighbors=50,weights = "uniform")
sklearn.model_selection.GridSearchCV
#knn = KNeighborsClassifier(n_neighbors=3)
#knn.fit(features,label1)

print(X_train)
print(y_train)
knn_clf=KNeighborsClassifier(n_neighbors=500,weights = "distance")
knn_clf.fit(X_train,y_train)
ypred=knn_clf.predict(X_test) 
test_pred = knn_clf.predict(X_test)
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("KNN Accuracy:",metrics.accuracy_score(y_test, test_pred))
print("KNN R2:",metrics.r2_score(y_test, test_pred))


from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
print("RF Accuracy:",metrics.accuracy_score(y_test, test_pred))
print("RF R2:",metrics.r2_score(y_test, test_pred))

# evaluate extra trees algorithm for classification
from sklearn.ensemble import ExtraTreesClassifier
clf = ExtraTreesClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print("Extra Tree Accuracy:",metrics.accuracy_score(y_test, test_pred))
print("Extra Tree R2:",metrics.r2_score(y_test, test_pred))